{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGJb1ScIJE-J"
   },
   "outputs": [],
   "source": [
    "# Google Colabでライブラリをアップロードする\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI3b2MALJQG9"
   },
   "outputs": [],
   "source": [
    "# Google Colabでドライブのデータを使う\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P9aOSopJUiL"
   },
   "outputs": [],
   "source": [
    "# Google Colabでライブラリをインストールする\n",
    "!pip install japanize_matplotlib bottleneck tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "id": "1Lq-ZkyCJJbu"
   },
   "outputs": [],
   "source": [
    "# 自作関数\n",
    "import MasterResearchFunction as mr\n",
    "\n",
    "# 基本ライブラリ\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import statistics\n",
    "from datetime import datetime, timedelta\n",
    "from decimal import Decimal\n",
    "\n",
    "# 数値計算とデータ処理\n",
    "import bottleneck as bn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 機械学習ライブラリ\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# ディープラーニングライブラリ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Add,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling1D,\n",
    "    Input,\n",
    "    LayerNormalization,\n",
    "    LSTM,\n",
    "    Masking,\n",
    "    MaxPooling1D,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# プロットと可視化\n",
    "import japanize_matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# その他のライブラリ\n",
    "from fastdtw import fastdtw\n",
    "from scipy import signal, stats\n",
    "from scipy.interpolate import interp1d, UnivariateSpline\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from tslearn.metrics import dtw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定数の定義\n",
    "NAMES = ['sakamoto', 'watabe', 'kotera', 'nakazawa', 'mizuno', 'todaka', 'takagawa', 'uchida']\n",
    "LABELS = ['circle', 'cross', 'tri', 'check']\n",
    "COLUMNS_TO_DROP = [\n",
    "    'Sensor', 'Participant name', 'Event', 'Event value',\n",
    "    'Eye movement type', 'Eye movement type index', 'Ungrouped',\n",
    "    'Validity left', 'Validity right', 'Gaze event duration', 'Gaze2D_Distance',\n",
    "    'Fixation_Distance', 'Gaze3D_Distance', 'Pupil_Diameter_Change',\n",
    "    'GazeDirection_Distance', 'PupilPosition_Distance'\n",
    "]\n",
    "BASE_DIR = \"/Users/hinase/CodeChord/MasterResearch/datasets/new\"\n",
    "Hz = 100\n",
    "#各被験者がジェスチャを行った順番(NAMES順)\n",
    "ROUTINE_LABELS = [\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 4, 3, 1],\n",
    "    [4, 3, 1, 2],\n",
    "    [2, 4, 3, 1],\n",
    "    [1, 4, 3, 2],\n",
    "    [3, 2, 1, 4],\n",
    "    [3, 1, 2, 4],\n",
    "    [2, 3, 1, 4]\n",
    "]\n",
    "# 数字からラベルへの変換マッピング\n",
    "label_map = {1: 'circle', 2: 'cross', 3: 'tri', 4: 'check'}\n",
    "# ラベル辞書を作成\n",
    "routine_label_dict = {name: [label_map[num] for num in labels] for name, labels in zip(NAMES, ROUTINE_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ジェスチャの教師データの読み込み\n",
    "trimmed_data_dict = mr.load_trimmed_gesture_data(NAMES, BASE_DIR)\n",
    "#教師データの最小の長さと最大の長さを格納\n",
    "length_extremes_dict = mr.calculate_length_extremes(trimmed_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#実験ルーティンのデータの読み込みと，各ジェスチャの実行部分のインデックスの抽出\n",
    "routine_data_dict = {}\n",
    "grouped_intervals = {}\n",
    "group_label_mapping = {}\n",
    "\n",
    "for name in NAMES:\n",
    "    print(f\"Processing {name}...\")\n",
    "\n",
    "    # データの読み込みと前処理\n",
    "    routine_data_dict[name] = mr.process_apple_watch_csv(f'{BASE_DIR}/{name}/motion/{name}_new_routine.csv')\n",
    "    a = mr.find_true_intervals(routine_data_dict[name])\n",
    "\n",
    "    # インターバルの開始インデックスを取得\n",
    "    start_indices = np.array([interval[0] for interval in a]).reshape(-1, 1)\n",
    "\n",
    "    # 階層的クラスタリング\n",
    "    Z = linkage(start_indices, method='ward')\n",
    "    cluster_labels = fcluster(Z, t=4, criterion='maxclust')  # クラスタ数を4に固定\n",
    "\n",
    "    # クラスタごとにインターバルをグループ化\n",
    "    interval_groups = {i: [] for i in range(1, 5)}\n",
    "    for interval, label in zip(a, cluster_labels):\n",
    "        interval_groups[label].append(interval)\n",
    "\n",
    "    # グループの順序をインターバルの開始インデックスに基づいてソート\n",
    "    sorted_groups = sorted(interval_groups.items(), key=lambda x: min(interval[0] for interval in x[1]))\n",
    "    sorted_interval_groups = {i + 1: group for i, (_, group) in enumerate(sorted_groups)}\n",
    "\n",
    "    # グループを保存\n",
    "    grouped_intervals[name] = sorted_interval_groups\n",
    "\n",
    "    # グループとラベルを関連付け\n",
    "    group_label_mapping[name] = {label: sorted_interval_groups[group_id]\n",
    "                                for group_id, label in enumerate(routine_label_dict[name], start=1)}\n",
    "\n",
    "    # 結果を出力\n",
    "    print(f\"\\n{name} のグループとラベル対応:\")\n",
    "    for label, intervals in group_label_mapping[name].items():\n",
    "        print(f\"  {label}: {intervals}\")\n",
    "\n",
    "# 'nakazawa' の 'check' に対応する最初のインデックスの組を削除\n",
    "if 'nakazawa' in group_label_mapping and 'check' in group_label_mapping['nakazawa']:\n",
    "    if len(group_label_mapping['nakazawa']['check']) > 0:\n",
    "        removed_interval = group_label_mapping['nakazawa']['check'].pop(0)\n",
    "        print(f\"'nakazawa' の 'check' から削除されたインターバル: {removed_interval}\")\n",
    "    else:\n",
    "        print(\"'nakazawa' の 'check' に削除するインターバルがありません。\")\n",
    "else:\n",
    "    print(\"'nakazawa' または 'check' に対応するデータが見つかりません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ジェスチャ実行時の-1秒から+1秒の範囲を抜き出す\n",
    "extracted_data_dict = {}\n",
    "\n",
    "for name in NAMES:\n",
    "    print(f\"Processing {name}...\")\n",
    "    extracted_data_dict[name] = {}\n",
    "\n",
    "    for label in LABELS:\n",
    "        print(f\"Processing label: {label}...\")\n",
    "\n",
    "        # 該当するインターバルを取得\n",
    "        intervals = group_label_mapping[name][label]\n",
    "\n",
    "        # 最初のインデックスと最後のインデックスを計算\n",
    "        start_idx = intervals[0][0] - 500\n",
    "        end_idx = intervals[-1][1] + 500\n",
    "\n",
    "        # インデックスの範囲を調整（負のインデックス防止）\n",
    "        start_idx = max(0, start_idx)\n",
    "        end_idx = min(len(routine_data_dict[name]), end_idx)\n",
    "\n",
    "        # データを抜き出し\n",
    "        extracted_data = routine_data_dict[name].iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "\n",
    "        # 抜き出したデータを辞書に格納\n",
    "        extracted_data_dict[name][label] = extracted_data\n",
    "\n",
    "        print(f\"Extracted data for {name} - {label}: start={start_idx}, end={end_idx}, length={len(extracted_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロットの関数\n",
    "def plot_extracted_data(extracted_data_dict, name, label):\n",
    "    \"\"\"\n",
    "    抽出データをプロットし、Marking が True の部分をハイライト。\n",
    "\n",
    "    Parameters:\n",
    "    - extracted_data_dict (dict): 抽出されたデータの辞書。\n",
    "    - name (str): 被験者の名前。\n",
    "    - label (str): ジェスチャーのラベル。\n",
    "    \"\"\"\n",
    "    data = extracted_data_dict[name][label]\n",
    "\n",
    "    if data.empty:\n",
    "        print(f\"{name} の {label} にデータがありません。\")\n",
    "        return\n",
    "\n",
    "    # 各特徴量をプロット\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.plot(data['AccelerationX'], label='AccelerationX', color='red', alpha=0.7)\n",
    "    plt.plot(data['AccelerationY'], label='AccelerationY', color='green', alpha=0.7)\n",
    "    plt.plot(data['AccelerationZ'], label='AccelerationZ', color='blue', alpha=0.7)\n",
    "    plt.plot(data['EuclideanNorm'], label='EuclideanNorm', color='purple', linewidth=1.5)\n",
    "\n",
    "    # Marking が True の箇所をハイライト\n",
    "    marking = data['Marking']\n",
    "    for i in range(len(marking)):\n",
    "        if marking.iloc[i]:\n",
    "            plt.axvspan(i, i + 1, color='yellow', alpha=0.3)\n",
    "\n",
    "    # ラベルとタイトル\n",
    "    plt.title(f\"{name} - {label}\", fontsize=16)\n",
    "    plt.xlabel(\"Time Step\", fontsize=12)\n",
    "    plt.ylabel(\"Value\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "    # プロット表示\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# プロット実行\n",
    "for name in NAMES:\n",
    "    for label in LABELS:\n",
    "        print(f\"Plotting {name} - {label}...\")\n",
    "        plot_extracted_data(extracted_data_dict, name, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS[jesutya]\n",
    "cross27.5\n",
    "check4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES[namae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS[jesutya]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namae = 0\n",
    "jesutya = 2\n",
    "Th = 9.1\n",
    "Ths = [Th, Th, Th]\n",
    "b = mr.three_axis_spring(extracted_data_dict[NAMES[namae]][LABELS[jesutya]], trimmed_data_dict[NAMES[namae]][LABELS[jesutya]], Ths, 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = mr.filter_and_combine_segments(b, Hz, min_time=length_extremes_dict[NAMES[namae]][LABELS[jesutya]]['min_length']*0.5/Hz, max_time=length_extremes_dict[NAMES[namae]][LABELS[jesutya]]['max_length']*2.5/Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフプロット関数\n",
    "def plot_detected_segments_with_highlights(data, marking_intervals, detected_intervals):\n",
    "    \"\"\"\n",
    "    データの区間をハイライトしてプロットする関数。\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): プロットするデータ。\n",
    "    - marking_intervals (list of tuple): ['Marking']がTrueであるインデックスのリスト。\n",
    "    - detected_intervals (list of tuple): SPRINGで検出されたインデックスのリスト。\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # 元データをプロット\n",
    "    plt.plot(data.index, data['EuclideanNorm'], label='EuclideanNorm', color='blue')\n",
    "\n",
    "    # ['Marking']がTrueの区間を黄色でハイライト\n",
    "    for start, end in marking_intervals:\n",
    "        plt.axvspan(start, end, color='yellow', alpha=0.3, label='Marking=True' if start == marking_intervals[0][0] else \"\")\n",
    "\n",
    "    # SPRINGで検出された区間を青色でハイライト\n",
    "    for start, end in detected_intervals:\n",
    "        plt.axvspan(start, end, color='cyan', alpha=0.4, label='SPRING Detected' if start == detected_intervals[0][0] else \"\")\n",
    "\n",
    "    # グラフ設定\n",
    "    plt.title(\"Detected Segments with Highlights\", fontsize=16)\n",
    "    plt.xlabel(\"Time Step\", fontsize=12)\n",
    "    plt.ylabel(\"EuclideanNorm\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# マーキング区間の抽出\n",
    "marking_intervals = []\n",
    "current_start = None\n",
    "for i in range(len(extracted_data_dict[NAMES[namae]][LABELS[jesutya]]['Marking'])):\n",
    "    if extracted_data_dict[NAMES[namae]][LABELS[jesutya]]['Marking'].iloc[i] and current_start is None:\n",
    "        current_start = i\n",
    "    elif not extracted_data_dict[NAMES[namae]][LABELS[jesutya]]['Marking'].iloc[i] and current_start is not None:\n",
    "        marking_intervals.append((current_start, i - 1))\n",
    "        current_start = None\n",
    "if current_start is not None:\n",
    "    marking_intervals.append((current_start, len(extracted_data_dict[NAMES[namae]][LABELS[jesutya]]['Marking']) - 1))\n",
    "\n",
    "# 検出された区間\n",
    "detected_intervals = c\n",
    "\n",
    "# グラフプロット\n",
    "plot_detected_segments_with_highlights(\n",
    "    extracted_data_dict[NAMES[namae]][LABELS[jesutya]], marking_intervals, detected_intervals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データとラベルを格納するリストの初期化\n",
    "X_filled = []\n",
    "y_labels = []\n",
    "true_labels = []\n",
    "# データとラベルを格納する辞書の初期化\n",
    "data_dict = {}\n",
    "label_dict = {}\n",
    "true_label_dict = {}\n",
    "\n",
    "for name in NAMES:\n",
    "    print(f\"Processing {name}...\")\n",
    "\n",
    "    # データの読み込みと前処理\n",
    "    eye_data, gesture_labels = mr.load_gesture_eye_data_pickle(name, LABELS, COLUMNS_TO_DROP)\n",
    "    data_dict[name] = {'gesture': eye_data, 'non_gesture': []}\n",
    "    label_dict[name] = {'gesture': gesture_labels, 'non_gesture': []}\n",
    "    true_label_dict[name] = {'gesture': ['gesture'] * len(eye_data), 'non_gesture': []}\n",
    "\n",
    "    # モーションデータとアイデータの読み込み\n",
    "    motion_routine_data, eye_routine_data = mr.load_motion_and_eye_data(name)\n",
    "    eye_non_gesture_data = mr.process_tobii_csv(f'datasets/new/{name}/eye/{name}_non_gesture.csv')\n",
    "\n",
    "    # gestureデータの長さを算出\n",
    "    if isinstance(eye_data, list):\n",
    "        lengths = [df.shape[0] for df in eye_data]  # 各データフレームの行数を取得\n",
    "        longest_length = max(lengths)\n",
    "        shortest_length = min(lengths)\n",
    "    else:\n",
    "        longest_length = eye_data.shape[0]\n",
    "        shortest_length = eye_data.shape[0]\n",
    "\n",
    "    print(f\"{name} の gesture データの長さ: 最長 {longest_length}, 最短 {shortest_length}\")\n",
    "\n",
    "    # ランダムな長さでデータを抽出\n",
    "    NON_GESTURE = 180\n",
    "    random_non_gesture_data = []\n",
    "\n",
    "    # ランダムな長さでデータを抽出（重複を防ぐために利用可能なインデックスを管理）\n",
    "    available_indices = list(range(eye_non_gesture_data.shape[0]))\n",
    "    random_non_gesture_data = []\n",
    "\n",
    "    for _ in range(NON_GESTURE):\n",
    "        # ランダムな長さを決定\n",
    "        random_length = random.randint(shortest_length, longest_length)\n",
    "\n",
    "        # 利用可能なインデックスからランダムに開始位置を選択\n",
    "        possible_start_indices = [\n",
    "            idx for idx in available_indices if idx + random_length <= len(available_indices)\n",
    "        ]\n",
    "        if not possible_start_indices:\n",
    "            print(f\"{name} の non-gesture データに十分なインデックスがありません。\")\n",
    "            break\n",
    "\n",
    "        start_idx = random.choice(possible_start_indices)\n",
    "        end_idx = start_idx + random_length\n",
    "\n",
    "        # 抽出したデータを追加\n",
    "        random_non_gesture_data.append(eye_non_gesture_data.iloc[start_idx:end_idx])\n",
    "\n",
    "        # 使用済みインデックスを削除\n",
    "        available_indices = [idx for idx in available_indices if idx < start_idx or idx >= end_idx]\n",
    "\n",
    "    # 抽出したデータを辞書に格納\n",
    "    data_dict[name]['non_gesture'] = random_non_gesture_data\n",
    "    label_dict[name]['non_gesture'] = ['non_gesture'] * len(random_non_gesture_data)\n",
    "    true_label_dict[name]['non_gesture'] = ['non_gesture'] * len(random_non_gesture_data)\n",
    "\n",
    "    print(f\"{name} の non-gesture データから {len(random_non_gesture_data)} 個のランダムデータを抽出しました。\")\n",
    "\n",
    "    # ジェスチャーデータの統合\n",
    "    for seq, lbl, true_lbl in zip(eye_data, gesture_labels, true_label_dict[name]['gesture']):\n",
    "        if seq is not None and len(seq) > 0:\n",
    "            X_filled.append(seq)\n",
    "            y_labels.append(lbl)\n",
    "            true_labels.append(true_lbl)\n",
    "        else:\n",
    "            print(f\"{name} のジェスチャーシーケンスが空です。対応するラベルをスキップします。\")\n",
    "\n",
    "    # 非ジェスチャーデータの統合\n",
    "    for seq, lbl, true_lbl in zip(data_dict[name]['non_gesture'], label_dict[name]['non_gesture'], true_label_dict[name]['non_gesture']):\n",
    "        if isinstance(seq, pd.DataFrame):\n",
    "            if not seq.empty:\n",
    "                X_filled.append(seq)\n",
    "                y_labels.append(lbl)\n",
    "                true_labels.append(true_lbl)\n",
    "            else:\n",
    "                print(f\"{name} の非ジェスチャーシーケンスが空です。対応するラベルをスキップします。\")\n",
    "        else:\n",
    "            print(f\"{name} の非ジェスチャーシーケンスが DataFrame ではありません。スキップします。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"総データ数: {len(X_filled)}\")\n",
    "print(f\"総ラベル数（y_labels）: {len(y_labels)}\")\n",
    "print(f\"総真偽ラベル数（true_labels）: {len(true_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データとラベルを格納するリストの初期化\n",
    "X_filled = []\n",
    "y_labels = []\n",
    "true_labels = []\n",
    "# データとラベルを格納する辞書の初期化\n",
    "data_dict = {}\n",
    "label_dict = {}\n",
    "true_label_dict = {}\n",
    "\n",
    "for name in NAMES:\n",
    "    print(f\"Processing {name}...\")\n",
    "\n",
    "    # データの読み込みと前処理\n",
    "    eye_data, gesture_labels = mr.load_gesture_eye_data_pickle(name, LABELS, COLUMNS_TO_DROP)\n",
    "    # 変数に保存\n",
    "    data_dict[name] = {\n",
    "        'gesture': eye_data,\n",
    "        'non_gesture': []\n",
    "    }\n",
    "    label_dict[name] = {\n",
    "        'gesture': gesture_labels,\n",
    "        'non_gesture': []\n",
    "    }\n",
    "    true_label_dict[name] = {\n",
    "        'gesture': ['gesture'] * len(eye_data),\n",
    "        'non_gesture': []\n",
    "    }\n",
    "\n",
    "    # モーションデータとアイデータの読み込みと前処理\n",
    "    motion_routine_data, eye_routine_data = mr.load_motion_and_eye_data(name)\n",
    "    eye_non_gesture_data = mr.process_tobii_csv(f'datasets/new/{name}/eye/{name}_non_gesture.csv')\n",
    "    # # 変数に保存\n",
    "    # motion_data = motion_routine_data\n",
    "\n",
    "    # # 除外インターバルの取得\n",
    "    # exclusion_intervals = mr.get_exclusion_intervals(motion_data)\n",
    "\n",
    "    # # データ全体の時間範囲の取得\n",
    "    # start_time = motion_routine_data['Timestamp'].min()\n",
    "    # end_time = motion_routine_data['Timestamp'].max()\n",
    "\n",
    "    # # 利用可能なインターバルの取得\n",
    "    # available_intervals = mr.get_available_intervals(exclusion_intervals, start_time, end_time)\n",
    "\n",
    "    # print(f\"{name} の利用可能なインターバル数: {len(available_intervals)}\")\n",
    "\n",
    "    # # ランダムなインターバルの抽出（eye_routine_data を使用）\n",
    "    # extracted_data = mr.extract_random_intervals(\n",
    "    #     eye_routine_data, available_intervals, MIN_INTERVAL, MAX_INTERVAL, NUM_INTERVALS\n",
    "    # )\n",
    "\n",
    "    # # 抽出されたデータを辞書に保存\n",
    "    # data_dict[name]['non_gesture'] = extracted_data\n",
    "    # label_dict[name]['non_gesture'] = ['non_gesture'] * len(extracted_data)\n",
    "    # true_label_dict[name]['non_gesture'] = ['non_gesture'] * len(extracted_data)\n",
    "\n",
    "    # # ジェスチャーデータの統合\n",
    "    # for seq, lbl, true_lbl in zip(eye_data, gesture_labels, true_label_dict[name]['gesture']):\n",
    "    #     if seq is not None and len(seq) > 0:\n",
    "    #         X_filled.append(seq)\n",
    "    #         y_labels.append(lbl)\n",
    "    #         true_labels.append(true_lbl)\n",
    "    #     else:\n",
    "    #         print(f\"{name} のジェスチャーシーケンスが空です。対応するラベルをスキップします。\")\n",
    "\n",
    "    # # 非ジェスチャーデータの統合\n",
    "    # for seq, lbl, true_lbl in zip(data_dict[name]['non_gesture'], label_dict[name]['non_gesture'], true_label_dict[name]['non_gesture']):\n",
    "    #     if isinstance(seq, pd.DataFrame):\n",
    "    #         if not seq.empty:\n",
    "    #             X_filled.append(seq)\n",
    "    #             y_labels.append(lbl)\n",
    "    #             true_labels.append(true_lbl)\n",
    "    #         else:\n",
    "    #             print(f\"{name} の非ジェスチャーシーケンスが空です。対応するラベルをスキップします。\")\n",
    "    #     else:\n",
    "    #         print(f\"{name} の非ジェスチャーシーケンスが DataFrame ではありません。スキップします。\")\n",
    "\n",
    "print(f\"総データ数: {len(X_filled)}\")\n",
    "print(f\"総ラベル数（y_labels）: {len(y_labels)}\")\n",
    "print(f\"総真偽ラベル数（true_labels）: {len(true_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイムスタンプのギャップを確認\n",
    "for name in NAMES:\n",
    "    non_gesture_intervals = data_dict[name]['non_gesture']\n",
    "    if non_gesture_intervals:\n",
    "        for idx, eye_routine_data in enumerate(non_gesture_intervals):\n",
    "            if isinstance(eye_routine_data, pd.DataFrame):\n",
    "                if not eye_routine_data.empty:\n",
    "                    time_diffs = eye_routine_data['Timestamp'].diff().dt.total_seconds()\n",
    "                    large_gaps = time_diffs[time_diffs > 0.05]\n",
    "                    print(f\"{name} の non_gesture_intervals[{idx}] の大きなタイムスタンプギャップ数: {len(large_gaps)}\")\n",
    "                else:\n",
    "                    print(f\"{name} の non_gesture_intervals[{idx}] が空です。\")\n",
    "            else:\n",
    "                print(f\"{name} の non_gesture_intervals[{idx}] が DataFrame ではありません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 前処理とフィルタリングの適用\n",
    "filtered_X, filtered_y, filtered_true = mr.preprocess_and_filter_sequences(\n",
    "    X_filled,\n",
    "    y_labels,\n",
    "    true_labels,\n",
    "    sampling_rate=50.0,\n",
    "    max_gap_seconds=0.1,\n",
    "    max_nan_seconds=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的なデータの確認\n",
    "print(f\"処理後のデータ数: {len(filtered_X)}\")\n",
    "print(f\"ラベル配列の長さ（y_labels_scaled）: {len(filtered_y)}\")\n",
    "print(f\"真偽ラベル配列の長さ（true_labels_scaled）: {len(filtered_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = []\n",
    "scaler = StandardScaler()\n",
    "for i in range(len(filtered_X)):\n",
    "    X_scaled.append(scaler.fit_transform(filtered_X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ラベルのエンコーディング（2値分類用に修正）\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(filtered_true)  # 0または1にエンコード\n",
    "\n",
    "# **ラベルのワンホットエンコーディングを削除**\n",
    "# 2値分類では必要ありません\n",
    "# y_categorical = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# 5. データの分割（パディングの前に行う）\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seq in enumerate(X_scaled):\n",
    "    print(f\"Sequence {i}: shape {seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 訓練データとテストデータでそれぞれ最大シーケンス長を計算\n",
    "max_length_train = max(len(seq) for seq in X_train_raw)\n",
    "max_length_test = max(len(seq) for seq in X_test_raw)\n",
    "max_length = max(max_length_train, max_length_test)\n",
    "\n",
    "# 7. シーケンスのパディング\n",
    "X_train_padded = pad_sequences(X_train_raw, maxlen=max_length, padding='post', value=0.0, dtype='float32')\n",
    "X_test_padded = pad_sequences(X_test_raw, maxlen=max_length, padding='post', value=0.0, dtype='float32')\n",
    "\n",
    "# 8. パディング後のデータに NaN が含まれていないか確認\n",
    "print('NaN in X_train_padded:', np.isnan(X_train_padded).any())\n",
    "print('NaN in X_test_padded:', np.isnan(X_test_padded).any())\n",
    "\n",
    "# 必要に応じて NaN を 0 で置換\n",
    "if np.isnan(X_train_padded).any():\n",
    "    X_train_padded = np.nan_to_num(X_train_padded, nan=0.0)\n",
    "if np.isnan(X_test_padded).any():\n",
    "    X_test_padded = np.nan_to_num(X_test_padded, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    固定のサイン・コサイン位置エンコーディングを生成する関数。\n",
    "\n",
    "    Parameters:\n",
    "    - max_len (int): シーケンスの最大長\n",
    "    - d_model (int): 埋め込み次元数\n",
    "\n",
    "    Returns:\n",
    "    - pos_encoding (np.array): 位置エンコーディング行列\n",
    "    \"\"\"\n",
    "    angle_rads = np.arange(max_len)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :]//2)) / np.float32(d_model))\n",
    "\n",
    "    # 偶数次元はsin、奇数次元はcos\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]  # (1, max_len, d_model)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "from tensorflow.keras.layers import Masking\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    \"\"\"\n",
    "    Transformerエンコーダーブロックを構築する関数。\n",
    "    \"\"\"\n",
    "    # 自己注意機構（マスクは自動的に適用される）\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs + x)\n",
    "\n",
    "    # フィードフォワードネットワーク\n",
    "    x_ff = Dense(ff_dim, activation='relu')(x)\n",
    "    x_ff = Dense(inputs.shape[-1])(x_ff)\n",
    "    x_ff = Dropout(dropout)(x_ff)\n",
    "    out = LayerNormalization(epsilon=1e-6)(x + x_ff)\n",
    "    return out\n",
    "\n",
    "def build_transformer_model(max_length, num_features, head_size, num_heads, ff_dim, num_transformer_blocks, dropout):\n",
    "    \"\"\"\n",
    "    Transformerベースの2値分類モデルを構築する関数。\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(max_length, num_features))\n",
    "\n",
    "    # マスキングレイヤーの追加（パディング値が0の場合）\n",
    "    x = Masking(mask_value=0.0)(inputs)\n",
    "\n",
    "    # 位置エンコーディングの追加\n",
    "    pos_encoding = get_positional_encoding(max_length, num_features)\n",
    "    x = x + pos_encoding[:,:max_length, :]\n",
    "\n",
    "    # Transformerエンコーダーブロックの追加\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    # グローバルプーリング\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    # 出力層\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの構築\n",
    "model = build_transformer_model(\n",
    "    max_length=max_length,\n",
    "    num_features=X_train_padded.shape[2],\n",
    "    head_size=64,\n",
    "    num_heads=8,\n",
    "    ff_dim=128,\n",
    "    num_transformer_blocks=1,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# モデルのサマリー表示\n",
    "model.summary()\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# モデルの訓練\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. モデルの評価\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 5. 予測と結果の表示\n",
    "y_pred_prob = model.predict(X_test_padded)\n",
    "y_pred_classes = (y_pred_prob > 0.5).astype(int).flatten()  # 閾値0.5でクラスを決定\n",
    "y_true_classes = y_test\n",
    "\n",
    "# ラベルを逆変換（元のクラス名に戻す場合）\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "\n",
    "# 分類レポートの表示\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列の作成と可視化\n",
    "conf_mat = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 特徴量数を取得\n",
    "num_features = X_train_padded.shape[2]\n",
    "print(f\"特徴量数: {num_features}\")\n",
    "\n",
    "# 2. モデルの構築（2値分類用に修正）\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=(max_length, num_features)))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))  # 出力層を1ユニットのシグモイドに変更\n",
    "\n",
    "# 3. モデルのコンパイルと学習\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_padded, y_test)\n",
    ")\n",
    "\n",
    "# 4. モデルの評価\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 5. 予測と結果の表示\n",
    "y_pred_prob = model.predict(X_test_padded)\n",
    "y_pred_classes = (y_pred_prob > 0.5).astype(int).flatten()  # 閾値0.5でクラスを決定\n",
    "y_true_classes = y_test\n",
    "\n",
    "# ラベルを逆変換（元のクラス名に戻す場合）\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "\n",
    "# 分類レポートの表示\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量数を取得\n",
    "num_features = X_train_padded.shape[2]\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. モデルの構築\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=(max_length, num_features)))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "# 5. モデルのコンパイルと学習\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_padded, y_test)\n",
    ")\n",
    "\n",
    "# 6. モデルの評価\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 7. 予測と結果の表示\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (max_length, num_features)\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Masking(mask_value=0.)(inputs)\n",
    "\n",
    "# num_featuresを定義（入力データの特徴量の次元数）\n",
    "num_features = input_shape[1]\n",
    "\n",
    "# Transformerブロックの定義\n",
    "def transformer_block(x, num_heads, key_dim, ff_dim, rate=0.1):\n",
    "    # マルチヘッド注意機構\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "\n",
    "    # 出力次元をnum_featuresに変換\n",
    "    attn_output = Dense(num_features)(attn_output)\n",
    "\n",
    "    out1 = Add()([x, attn_output])\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "\n",
    "    # フィードフォワードネットワーク\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "\n",
    "    # 出力次元をnum_featuresに変換\n",
    "    ffn_output = Dense(num_features)(ffn_output)\n",
    "\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    out2 = Add()([out1, ffn_output])\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out2)\n",
    "    return out2\n",
    "\n",
    "# Transformerブロックの適用\n",
    "x = transformer_block(x, num_heads=4, key_dim=64, ff_dim=128)\n",
    "\n",
    "# プーリングと出力層\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "# モデルの作成\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. モデルのコンパイルと学習\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_padded, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. モデルの評価\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 7. 予測と結果の表示\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 8. 特徴量の重要度評価（Permutation Feature Importance）\n",
    "\n",
    "# 8.1 ベースラインの性能を計算\n",
    "# テストデータでの予測\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# ベースラインのAccuracyを計算\n",
    "baseline_accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "print(f'Baseline Accuracy: {baseline_accuracy:.4f}')\n",
    "\n",
    "# 8.2 特徴量ごとの重要度を計算\n",
    "num_features = X_test_padded.shape[2]\n",
    "feature_importances = []\n",
    "\n",
    "for feature_idx in range(num_features):\n",
    "    # テストデータをコピー\n",
    "    X_test_permuted = X_test_padded.copy()\n",
    "\n",
    "    # 特徴量をランダムにシャッフル（サンプル間でシャッフル）\n",
    "    permuted_feature = X_test_permuted[:, :, feature_idx].reshape(X_test_permuted.shape[0], -1)\n",
    "    np.random.shuffle(permuted_feature)\n",
    "    X_test_permuted[:, :, feature_idx] = permuted_feature.reshape(X_test_permuted.shape[0], X_test_permuted.shape[1])\n",
    "\n",
    "    # シャッフル後のデータで予測\n",
    "    y_pred_permuted = model.predict(X_test_permuted)\n",
    "    y_pred_classes_permuted = np.argmax(y_pred_permuted, axis=1)\n",
    "\n",
    "    # シャッフル後のAccuracyを計算\n",
    "    permuted_accuracy = accuracy_score(y_true_classes, y_pred_classes_permuted)\n",
    "\n",
    "    # 性能の差を計算（重要度）\n",
    "    importance = baseline_accuracy - permuted_accuracy\n",
    "    feature_importances.append(importance)\n",
    "    print(f'Feature {feature_idx + 1} Importance: {importance:.4f}')\n",
    "\n",
    "# 8.3 特徴量重要度の可視化\n",
    "feature_names = [f'Feature {i+1}' for i in range(num_features)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X[7].columns, feature_importances)\n",
    "plt.xlabel('Decrease in Accuracy')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHbr81KP4EsX"
   },
   "outputs": [],
   "source": [
    "#  pickleファイルを読み込む\n",
    "with open('/Users/hinase/Downloads/Th-s/d_nakazawa_acc_check_segments4.7new.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "segx = data['d_nakazawa_check_segx']\n",
    "segy = data['d_nakazawa_check_segy']\n",
    "segz = data['d_nakazawa_check_segz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple sine wave as an example of a waveform or signal\n",
    "x = np.linspace(0, 10, 1000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Define the sections to be colored\n",
    "color_sections = [(200, 300, 'red'), (450, 550, 'blue'), (700, 800, 'red')]\n",
    "\n",
    "# Plot the entire waveform in gray\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(x, y, color='gray', linewidth=3)\n",
    "\n",
    "# Highlight the specified sections with the chosen colors\n",
    "for start, end, color in color_sections:\n",
    "    plt.plot(x[start:end], y[start:end], color=color, linewidth=5)\n",
    "\n",
    "# Add an arrow to indicate the flow of data\n",
    "# plt.annotate('', xy=(10, 0), xytext=(0, 0),\n",
    "#              arrowprops=dict(facecolor='black', shrink=0.05, width=2))\n",
    "plt.xticks(color=\"None\")\n",
    "plt.yticks(color=\"None\")\n",
    "plt.tick_params(length=0)\n",
    "# Remove axes for a cleaner look\n",
    "plt.axis('off')\n",
    "#plt.savefig(\"/Users/hinase/Downloads/plt.svg\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cairosvg\n",
    "\n",
    "# SVGファイルのパス\n",
    "input_svg_path = \"/Users/hinase/Downloads/ss.svg\"\n",
    "# 出力するEPSファイルのパス\n",
    "output_eps_path = \"/Users/hinase/Downloads/ss.eps\"\n",
    "\n",
    "# SVGをEPSに変換\n",
    "cairosvg.svg2eps(url=input_svg_path, write_to=output_eps_path)\n",
    "\n",
    "print(f\"SVG画像がEPS形式で '{output_eps_path}' に保存されました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import openpyxl\n",
    "from natsort import natsorted\n",
    "\n",
    "folder_path = '/Users/hinase/Downloads/folder4'\n",
    "folder_2_path = '/Users/hinase/Downloads/folder5'\n",
    "\n",
    "# Get Excel file names in natural order, filtering only supported Excel files\n",
    "supported_extensions = ('.xlsx', '.xlsm', '.xltx', '.xltm')\n",
    "files = natsorted([f for f in os.listdir(folder_path) if f.endswith(supported_extensions)])\n",
    "\n",
    "# Loop over filenames and build absolute path for each file\n",
    "for filename in files:\n",
    "    filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Access xlsx file and get the first sheet object\n",
    "    wb = openpyxl.load_workbook(filepath)\n",
    "    ws_name = wb.sheetnames[0]\n",
    "    ws = wb[ws_name]\n",
    "\n",
    "    # Convert to csv and save to folder4\n",
    "    savecsv_path = os.path.join(folder_2_path, filename.rstrip(\".xlsx\") + \".csv\")\n",
    "    with open(savecsv_path, 'w', newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in ws.rows:\n",
    "            writer.writerow([cell.value for cell in row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tslearn.metrics import dtw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data.csv\", header=None)[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:1000:4]\n",
    "Y = data[1000::4]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(X, label=\"Stream Data\")\n",
    "plt.plot(Y, label=\"Test Data\")\n",
    "plt.legend()\n",
    "# PDFで保存\n",
    "plt.savefig(\"/Users/hinase/Downloads/spring.pdf\", format=\"pdf\")  # PDF形式で保存\n",
    "plt.show()  # グラフを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spring import spring\n",
    "plt.figure(figsize=(6, 5))\n",
    "# ループ内では凡例を設定せず、プロットのみ行う\n",
    "for path, cost in spring(X, Y, 80):\n",
    "    plt.plot(X, c=\"gray\", alpha=0.5)  # 灰色の線\n",
    "    plt.plot(path[:, 0], X[path[:, 0]], c=\"C2\")  # 緑色の線\n",
    "\n",
    "# ループの外で凡例を1回だけ設定\n",
    "plt.plot([], [], c=\"C2\", label=\"Segments identified as gesture performed\")  # ダミーラインで凡例を作成\n",
    "plt.legend(loc=\"upper right\")  # 凡例を右上に固定\n",
    "plt.savefig(\"/Users/hinase/Downloads/afterspring.pdf\", format=\"pdf\")  # PDF形式で保存\n",
    "plt.show()  # グラフを表示"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.9.19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
