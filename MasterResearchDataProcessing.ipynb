{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGJb1ScIJE-J"
   },
   "outputs": [],
   "source": [
    "# Google Colabでライブラリをアップロードする\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI3b2MALJQG9"
   },
   "outputs": [],
   "source": [
    "# Google Colabでドライブのデータを使う\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P9aOSopJUiL"
   },
   "outputs": [],
   "source": [
    "# Google Colabでライブラリをインストールする\n",
    "!pip install japanize_matplotlib bottleneck tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Lq-ZkyCJJbu"
   },
   "outputs": [],
   "source": [
    "# 自作関数\n",
    "import MasterResearchFunction as mr\n",
    "\n",
    "# 基本ライブラリ\n",
    "import os, re, csv, math, statistics\n",
    "from datetime import datetime, timedelta\n",
    "from decimal import Decimal\n",
    "import pickle\n",
    "\n",
    "# 数値計算とデータ処理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bottleneck as bn\n",
    "\n",
    "# 機械学習ライブラリ\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ディープラーニングライブラリ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "# プロットと可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import japanize_matplotlib\n",
    "\n",
    "# その他のライブラリ\n",
    "from scipy import signal, stats\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.interpolate import interp1d\n",
    "from tslearn.metrics import dtw_path\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用のデータを定義\n",
    "G = [1, 2, 0, 2, 1, 2, 4, 2, 1, 0, 2, 0, 2, 0, 2, 4, 2, 1, 1, 4, 3, 2, 1, 2, 4, 0, 2, 1, 1, 2]  # 対象の時間シリーズデータ\n",
    "QG = [1, 2, 1]  # クエリ時間シリーズデータ\n",
    "Th = 1  # しきい値\n",
    "\n",
    "segments = mr.spring(G, QG, Th)\n",
    "\n",
    "print(\"検出されたセグメント:\")\n",
    "for seg in segments:\n",
    "    print(f\"開始位置: {seg[0]}, 累積距離: {seg[1]}, 開始時刻: {seg[2]}, 終了時刻: {seg[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSPIQYV9JE-O"
   },
   "outputs": [],
   "source": [
    "#Apple Watchのモーションデータの読み込み\n",
    "a_yuuma_motion_data = mr.process_apple_watch_csv(\"datasets/yuuma/20240604/yuuma_motion.csv\")\n",
    "b_sakamoto_motion_data = mr.process_apple_watch_csv(\"datasets/sakamoto/20240604/sakamoto_motion.csv\")\n",
    "c_watabe_motion_data = mr.process_apple_watch_csv(\"datasets/watabe/20240605/watabe_motion.csv\")\n",
    "d_nakazawa_motion_data = mr.process_apple_watch_csv(\"datasets/nakazawa/20240606/nakazawa_motion.csv\")\n",
    "e_okede_motion_data = mr.process_apple_watch_csv(\"datasets/okeda/20240607/okeda_motion.csv\")\n",
    "# a_yuuma_motion_data = mr.process_apple_watch_csv(\"/content/drive/MyDrive/datasets/yuuma/20240604/yuuma_motion.csv\")\n",
    "# b_sakamoto_motion_data = mr.process_apple_watch_csv(\"/content/drive/MyDrive/datasets/sakamoto/20240604/sakamoto_motion.csv\")\n",
    "# c_watabe_motion_data = mr.process_apple_watch_csv(\"/content/drive/MyDrive/datasets/watabe/20240605/watabe_motion.csv\")\n",
    "# d_nakazawa_motion_data = mr.process_apple_watch_csv(\"/content/drive/MyDrive/datasets/nakazawa/20240606/nakazawa_motion.csv\")\n",
    "# e_okeda_motion_data = mr.process_apple_watch_csv(\"/content/drive/MyDrive/datasets/okeda/20240607/okeda_motion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwuLVlktIfm9"
   },
   "outputs": [],
   "source": [
    "#Tobiiのモーションデータの読み込み\n",
    "a_yuuma_eye_data = mr.process_tobii_csv(\"datasets/yuuma/20240604/yuuma_eye.csv\")\n",
    "b_sakamoto_eye_data = mr.process_tobii_csv(\"datasets/sakamoto/20240604/sakamoto_eye.csv\")\n",
    "c_watabe_eye_data = mr.process_tobii_csv(\"datasets/watabe/20240605/watabe_eye.csv\")\n",
    "d_nakazawa_eye_data = mr.process_tobii_csv(\"datasets/nakazawa/20240606/nakazawa_eye.csv\")\n",
    "e_okeda_eye_data = mr.process_tobii_csv(\"datasets/okeda/20240607/okeda_eye.csv\")\n",
    "# a_yuuma_eye_data = mr.process_tobii_csv(\"/content/drive/MyDrive/datasets/yuuma/20240604/yuuma_eye.csv\")\n",
    "# b_sakamoto_eye_data = mr.process_tobii_csv(\"/content/drive/MyDrive/datasets/sakamoto/20240604/sakamoto_eye.csv\")\n",
    "# c_watabe_eye_data = mr.process_tobii_csv(\"/content/drive/MyDrive/datasets/watabe/20240605/watabe_eye.csv\")\n",
    "# d_nakazawa_eye_data = mr.process_tobii_csv(\"/content/drive/MyDrive/datasets/nakazawa/20240606/nakazawa_eye.csv\")\n",
    "# e_okeda_eye_data = mr.process_tobii_csv(\"/content/drive/MyDrive/datasets/okeda/20240607/okeda_eye.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOEsc9-3fnXT"
   },
   "outputs": [],
   "source": [
    "#a_yuumaくんの教師データ読み込み\n",
    "a_yuuma_check = mr.process_all_apple_watch_csv_in_directory(\"datasets/yuuma/train_gesture/check\")\n",
    "a_yuuma_circle = mr.process_all_apple_watch_csv_in_directory(\"datasets/yuuma/train_gesture/circle\")\n",
    "a_yuuma_cross = mr.process_all_apple_watch_csv_in_directory(\"datasets/yuuma/train_gesture/cross(new)\")\n",
    "a_yuuma_tri = mr.process_all_apple_watch_csv_in_directory(\"datasets/yuuma/train_gesture/tri\")\n",
    "# a_yuuma_check = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/yuuma/train_gesture/check\")\n",
    "# a_yuuma_circle = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/yuuma/train_gesture/circle\")\n",
    "# a_yuuma_cross = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/yuuma/train_gesture/cross(new)\")\n",
    "# a_yuuma_tri = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/yuuma/train_gesture/tri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uK2kISJYI71I"
   },
   "outputs": [],
   "source": [
    "#b_sakamotoくんの教師データ読み込み\n",
    "b_sakamoto_check = mr.process_all_apple_watch_csv_in_directory(\"datasets/sakamoto/train_gesture(old)/check\")\n",
    "b_sakamoto_circle = mr.process_all_apple_watch_csv_in_directory(\"datasets/sakamoto/train_gesture(old)/circle\")\n",
    "b_sakamoto_cross = mr.process_all_apple_watch_csv_in_directory(\"datasets/sakamoto/train_gesture(old)/cross\")\n",
    "b_sakamoto_tri = mr.process_all_apple_watch_csv_in_directory(\"datasets/sakamoto/train_gesture(old)/tri\")\n",
    "# b_sakamoto_check = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/sakamoto/train_gesture(old)/check\")\n",
    "# b_sakamoto_circle = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/sakamoto/train_gesture(old)/circle\")\n",
    "# b_sakamoto_cross = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/sakamoto/train_gesture(old)/cross\")\n",
    "# b_sakamoto_tri = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/sakamoto/train_gesture(old)/tri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4J3r6Flc4EsT"
   },
   "outputs": [],
   "source": [
    "#b_sakamotoくんの教師データ読み込み\n",
    "b_sakamoto_check = mr.process_all_apple_watch_csv_in_directory(\"datasets/sakamoto/train_gesture(new)/check\")\n",
    "b_sakamoto_circle = mr.process_all_apple_watch_csv_in_directory(\"datasets/sakamoto/train_gesture(new)/circle\")\n",
    "b_sakamoto_cross = mr.process_all_apple_watch_csv_in_directory(\"datasets/sakamoto/train_gesture(new)/cross\")\n",
    "b_sakamoto_tri = mr.process_all_apple_watch_csv_in_directory(\"datasets/sakamoto/train_gesture(new)/tri\")\n",
    "# b_sakamoto_check = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/sakamoto/train_gesture(new)/check\")\n",
    "# b_sakamoto_circle = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/sakamoto/train_gesture(new)/circle\")\n",
    "# b_sakamoto_cross = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/sakamoto/train_gesture(new)/cross\")\n",
    "# b_sakamoto_tri = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/sakamoto/train_gesture(new)/tri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diEoQTluAkuW"
   },
   "outputs": [],
   "source": [
    "#c_watabeくんの教師データ読み込み\n",
    "c_watabe_check = mr.process_all_apple_watch_csv_in_directory(\"datasets/watabe/train_gesture/check\")\n",
    "c_watabe_circle = mr.process_all_apple_watch_csv_in_directory(\"datasets/watabe/train_gesture/circle\")\n",
    "c_watabe_cross = mr.process_all_apple_watch_csv_in_directory(\"datasets/watabe/train_gesture/cross\")\n",
    "c_watabe_tri = mr.process_all_apple_watch_csv_in_directory(\"datasets/watabe/train_gesture/tri\")\n",
    "# c_watabe_check = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/watabe/train_gesture/check\")\n",
    "# c_watabe_circle = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/watabe/train_gesture/circle\")\n",
    "# c_watabe_cross = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/watabe/train_gesture/cross\")\n",
    "# c_watabe_tri = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/watabe/train_gesture/tri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lE7CC2oFKDi8"
   },
   "outputs": [],
   "source": [
    "#d_nakazawaくんの教師データ読み込み\n",
    "d_nakazawa_check = mr.process_all_apple_watch_csv_in_directory(\"datasets/nakazawa/train_gesture/check\")\n",
    "d_nakazawa_circle = mr.process_all_apple_watch_csv_in_directory(\"datasets/nakazawa/train_gesture/circle\")\n",
    "d_nakazawa_cross = mr.process_all_apple_watch_csv_in_directory(\"datasets/nakazawa/train_gesture/cross\")\n",
    "d_nakazawa_tri = mr.process_all_apple_watch_csv_in_directory(\"datasets/nakazawa/train_gesture/tri\")\n",
    "# d_nakazawa_check = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/nakazawa/train_gesture/check\")\n",
    "# d_nakazawa_circle = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/nakazawa/train_gesture/circle\")\n",
    "# d_nakazawa_cross = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/nakazawa/train_gesture/cross\")\n",
    "# d_nakazawa_tri = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/nakazawa/train_gesture/tri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrS9POJEKLrV"
   },
   "outputs": [],
   "source": [
    "#e_okedaくんの教師データ読み込み\n",
    "e_okeda_check = mr.process_all_apple_watch_csv_in_directory(\"datasets/okeda/train_gesture/check\")\n",
    "e_okeda_circle = mr.process_all_apple_watch_csv_in_directory(\"datasets/okeda/train_gesture/circle\")\n",
    "e_okeda_cross = mr.process_all_apple_watch_csv_in_directory(\"datasets/okeda/train_gesture/cross\")\n",
    "e_okeda_tri = mr.process_all_apple_watch_csv_in_directory(\"datasets/okeda/train_gesture/tri\")\n",
    "# e_okeda_check = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/okeda/train_gesture/check\")\n",
    "# e_okeda_circle = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/okeda/train_gesture/circle\")\n",
    "# e_okeda_cross = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/okeda/train_gesture/cross\")\n",
    "# e_okeda_tri = mr.process_all_apple_watch_csv_in_directory(\"/content/drive/MyDrive/datasets/okeda/train_gesture/tri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data = mr.process_apple_watch_csv('datasets/new/kotera/motion/kotera_eye_check.csv')\n",
    "#motion_data = mr.process_apple_watch_csv('/Users/hinase/Downloads/MotionData_20241018_203400.csv')\n",
    "#train_data = mr.process_all_apple_watch_csv_in_directory('/Users/hinase/Downloads/circle')\n",
    "#eye_data = mr.process_tobii_csv('/Users/hinase/Downloads/folder2/watabe_circle.csv')\n",
    "eye_data = mr.process_tobii_csv('/Users/hinase/Downloads/folder2/kotera_check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data = mr.process_apple_watch_csv('datasets/new/sakamoto/motion/sakamoto_new_routine.csv')\n",
    "eye_data = mr.process_tobii_csv('datasets/new/sakamoto/eye/sakamoto_eye_new_routine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_data[\"Timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25秒減算坂本\n",
    "#eye_data[\"Timestamp\"] = eye_data[\"Timestamp\"] - pd.to_timedelta(25.64, unit='s')\n",
    "# 29秒減算渡部\n",
    "#eye_data[\"Timestamp\"] = eye_data[\"Timestamp\"] - pd.to_timedelta(29.766, unit='s')\n",
    "# 43秒減算小寺\n",
    "#eye_data[\"Timestamp\"] = eye_data[\"Timestamp\"] - pd.to_timedelta(43.1129, unit='s')\n",
    "# 43秒減算小寺(tri)\n",
    "eye_data[\"Timestamp\"] = eye_data[\"Timestamp\"] - pd.to_timedelta(43.04, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_data[\"Timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data[\"Timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Timestamp('2024-10-16 13:25:04.847080946')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hinase_circle_seg = mr.three_axis_spring(motion_data, train_data, [5, 5, 5], 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hz = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_true_intervals(df):\n",
    "    # 'Marking' カラムがTrueとなっている箇所を抽出\n",
    "    true_intervals = []\n",
    "    start_index = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Marking']:\n",
    "            if start_index is None:\n",
    "                start_index = index\n",
    "        else:\n",
    "            if start_index is not None:\n",
    "                true_intervals.append((start_index, index - 1))\n",
    "                start_index = None\n",
    "\n",
    "    # 最後のTrueの区間がデータフレームの最後まで続く場合\n",
    "    if start_index is not None:\n",
    "        true_intervals.append((start_index, df.index[-1]))\n",
    "\n",
    "    return true_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = find_true_intervals(motion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data[\"Timestamp\"][4679]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.append([a[i][0]+15, a[i][1]-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del b[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(b)):\n",
    "  print(b[i][1] - b[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a)):\n",
    "  print(a[i][1] - a[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del b[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in range(len(b)):\n",
    "  c.append([motion_data[\"Timestamp\"][b[i][0]], motion_data[\"Timestamp\"][b[i][1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in range(len(a)):\n",
    "  c.append([motion_data[\"Timestamp\"][a[i][0]], motion_data[\"Timestamp\"][a[i][1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 1. 除外するインターバルのデータフレーム作成\n",
    "intervals_df = pd.DataFrame(c, columns=['start', 'end'])\n",
    "intervals_df = intervals_df.sort_values('start').reset_index(drop=True)\n",
    "\n",
    "# 2. データ全体の時間範囲の取得\n",
    "start_time = eye_data['Timestamp'].min()\n",
    "end_time = eye_data['Timestamp'].max()\n",
    "\n",
    "# 3. 利用可能なインターバルの取得\n",
    "available_intervals = []\n",
    "\n",
    "if intervals_df.iloc[0]['start'] > start_time:\n",
    "    available_intervals.append([start_time, intervals_df.iloc[0]['start']])\n",
    "\n",
    "for i in range(len(intervals_df) - 1):\n",
    "    gap_start = intervals_df.iloc[i]['end']\n",
    "    gap_end = intervals_df.iloc[i + 1]['start']\n",
    "    if gap_end > gap_start:\n",
    "        available_intervals.append([gap_start, gap_end])\n",
    "\n",
    "if intervals_df.iloc[-1]['end'] < end_time:\n",
    "    available_intervals.append([intervals_df.iloc[-1]['end'], end_time])\n",
    "\n",
    "# 4. インターバルのパラメータ設定\n",
    "min_interval = 1.42  # 最小インターバル長（秒）\n",
    "max_interval = 2.68  # 最大インターバル長（秒）\n",
    "num_intervals = 80  # 抽出するインターバルの数\n",
    "\n",
    "ppp = []\n",
    "attempts = 0\n",
    "max_attempts = num_intervals * 10  # 最大試行回数\n",
    "\n",
    "while len(ppp) < num_intervals and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "\n",
    "    # 利用可能なインターバルをフィルタリング\n",
    "    suitable_intervals = [\n",
    "        interval for interval in available_intervals\n",
    "        if (interval[1] - interval[0]).total_seconds() >= min_interval\n",
    "    ]\n",
    "\n",
    "    if not suitable_intervals:\n",
    "        print(\"利用可能なインターバルが足りません。\")\n",
    "        break\n",
    "\n",
    "    avail_interval = random.choice(suitable_intervals)\n",
    "    avail_start, avail_end = avail_interval\n",
    "    avail_duration = (avail_end - avail_start).total_seconds()\n",
    "\n",
    "    max_possible_duration = min(avail_duration, max_interval)\n",
    "    duration = random.uniform(min_interval, max_possible_duration)\n",
    "\n",
    "    latest_start_time = avail_end - pd.Timedelta(seconds=duration)\n",
    "    if latest_start_time <= avail_start:\n",
    "        continue\n",
    "\n",
    "    random_offset = random.uniform(0, (latest_start_time - avail_start).total_seconds())\n",
    "    start = avail_start + pd.Timedelta(seconds=random_offset)\n",
    "    end = start + pd.Timedelta(seconds=duration)\n",
    "\n",
    "    data_in_interval = eye_data[(eye_data['Timestamp'] >= start) & (eye_data['Timestamp'] <= end)]\n",
    "\n",
    "    if not data_in_interval.empty:\n",
    "        ppp.append(data_in_interval)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(f\"抽出されたインターバル数: {len(ppp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 削除するカラムのリスト\n",
    "columns_to_drop = [\n",
    "    'Sensor', 'Participant name', 'Event', 'Event value',\n",
    "    'Eye movement type', 'Eye movement type index', 'Ungrouped', 'Timestamp',\n",
    "    'Validity left', 'Validity right', 'Gaze event duration', 'Gaze2D_Distance',\n",
    "    'Fixation_Distance', 'Gaze3D_Distance', 'Pupil_Diameter_Change',\n",
    "    'GazeDirection_Distance', 'PupilPosition_Distance'\n",
    "]\n",
    "\n",
    "# ppp内の各データフレームから指定されたカラムを削除\n",
    "ppp_cleaned = []\n",
    "\n",
    "for idx, df in enumerate(ppp):\n",
    "    # 存在しないカラムがある場合にエラーが出ないように、errors='ignore'を指定\n",
    "    df_dropped = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    ppp_cleaned.append(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各範囲内のデータを格納するリストを用意します\n",
    "result = []\n",
    "# cの各行に対してループを回します\n",
    "for start_time, end_time in c:\n",
    "    # 範囲内のデータを抽出するためのマスクを作成します\n",
    "    mask = (eye_data[\"Timestamp\"] >= start_time) & (eye_data[\"Timestamp\"] <= end_time)\n",
    "    # マスクを適用してデータを抽出します\n",
    "    data_in_range = eye_data.loc[mask]\n",
    "    # 結果をリストに追加します\n",
    "    result.append(data_in_range)\n",
    "\n",
    "# 結果を確認します\n",
    "for idx, data in enumerate(result):\n",
    "    print(f\"範囲 {idx+1}:\")\n",
    "    print(data)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultがリストである場合\n",
    "first_20_results = result[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickleファイルに出力する\n",
    "with open('/Users/hinase/Downloads/kotera_check_eye.pkl', 'wb') as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#names = ['kawano', 'sakamoto', 'watabe', 'nakazawa', 'kotera']\n",
    "names = ['kotera']\n",
    "#names = ['sakamoto', 'watabe', 'kotera']\n",
    "\n",
    "labels = ['circle', 'cross', 'tri', 'check']\n",
    "\n",
    "# 削除するカラムのリスト\n",
    "columns_to_drop = [\n",
    "    'Sensor', 'Participant name', 'Event', 'Event value',\n",
    "    'Eye movement type', 'Eye movement type index', 'Ungrouped', 'Timestamp',\n",
    "    'Validity left', 'Validity right', 'Gaze event duration', 'Gaze2D_Distance',\n",
    "    'Fixation_Distance', 'Gaze3D_Distance', 'Pupil_Diameter_Change',\n",
    "    'GazeDirection_Distance', 'PupilPosition_Distance'\n",
    "]\n",
    "\n",
    "for name in names:\n",
    "    eye_data = []\n",
    "    for label in labels:\n",
    "        filename = f'/Users/hinase/Downloads/{name}_{label}_eye.pkl'\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            eye_data.extend(data)\n",
    "    # 指定されたカラムを削除\n",
    "    processed_data = []\n",
    "    for df in eye_data:\n",
    "        processed_df = df.drop(columns=columns_to_drop)\n",
    "        processed_data.append(processed_df)\n",
    "    # 各人物ごとのデータを変数に保存\n",
    "    globals()[f'{name}_eye'] = processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト内包表記で一度にリストを作成\n",
    "for name in names:\n",
    "  label = [s for s in ['circle', 'cross', 'tri', 'check'] for _ in range(20)]\n",
    "  globals()[f'{name}_label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト'true_label'に'gesture'を80個格納\n",
    "true_label = ['gesture'] * 80\n",
    "# リスト'false_label'に'non-gesture'を80個格納\n",
    "false_label = ['non-gesture'] * 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既に names リストが定義されていると仮定します\n",
    "X = []\n",
    "label = []\n",
    "\n",
    "for name in names:\n",
    "    eye_data = globals()[f'{name}_eye']     # シーケンスのリスト\n",
    "    labels = globals()[f'{name}_label']     # 対応するラベルのリスト\n",
    "\n",
    "    for seq, lbl in zip(eye_data, labels):\n",
    "        # シーケンスが空でないか確認\n",
    "        if seq is not None and len(seq) > 0:\n",
    "            X.append(seq)\n",
    "            label.append(lbl)\n",
    "        else:\n",
    "            print(f\"{name} のシーケンスが空です。対応するラベルをスキップします。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "X.extend(sakamoto_eye)\n",
    "X.extend(ppp_cleaned)\n",
    "label=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.extend(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.extend(false_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split#データ分割用\n",
    "from sklearn.ensemble import RandomForestClassifier#ランダムフォレスト\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. パディング後のデータに NaN が含まれていないか確認\n",
    "print('NaN in X:', np.isnan(X).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filled = []\n",
    "X_scaled = []\n",
    "for idx, sequence in enumerate(X):\n",
    "    df = pd.DataFrame(sequence)\n",
    "    # 欠損値の補間と補完\n",
    "    df = df.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "    df = df.fillna(method='ffill')\n",
    "    df = df.fillna(method='bfill')\n",
    "    # 残る NaN を 0 で埋める\n",
    "    if df.isnull().values.any():\n",
    "        print(f\"シーケンス {idx} にまだ NaN が存在します。0 で埋めます。\")\n",
    "        df = df.fillna(0)\n",
    "    # データ型を数値型に変換（必要に応じて）\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    X_filled.append(df.values)\n",
    "\n",
    "# スケーリング前に NaN をチェック\n",
    "for idx, sequence in enumerate(X_filled):\n",
    "    if np.isnan(sequence).any():\n",
    "        print(f\"シーケンス {idx} に NaN が残っています。スケーリングをスキップします。\")\n",
    "        continue\n",
    "    if sequence.shape[0] > 0:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled.append(scaler.fit_transform(sequence))\n",
    "    else:\n",
    "        print(f\"シーケンス {idx} は空です。スケーリングをスキップします。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ラベルのエンコーディング\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(label)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_categorical = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# 2. データの分割（パディングの前に行う）\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_categorical, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. 訓練データとテストデータでそれぞれ最大シーケンス長を計算\n",
    "max_length_train = max(len(seq) for seq in X_train_raw)\n",
    "max_length_test = max(len(seq) for seq in X_test_raw)\n",
    "max_length = max(max_length_train, max_length_test)\n",
    "\n",
    "# 4. シーケンスのパディング\n",
    "X_train_padded = pad_sequences(X_train_raw, maxlen=max_length, padding='post', value=0.0, dtype='float32')\n",
    "X_test_padded = pad_sequences(X_test_raw, maxlen=max_length, padding='post', value=0.0, dtype='float32')\n",
    "\n",
    "# 5. パディング後のデータに NaN が含まれていないか確認\n",
    "print('NaN in X_train_padded:', np.isnan(X_train_padded).any())\n",
    "print('NaN in X_test_padded:', np.isnan(X_test_padded).any())\n",
    "\n",
    "# 必要に応じて NaN を 0 で置換\n",
    "if np.isnan(X_train_padded).any():\n",
    "    X_train_padded = np.nan_to_num(X_train_padded, nan=0.0)\n",
    "if np.isnan(X_test_padded).any():\n",
    "    X_test_padded = np.nan_to_num(X_test_padded, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of samples in X_scaled: {len(X_scaled)}')\n",
    "print(f'Number of samples in y_categorical: {len(y_categorical)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量数を取得\n",
    "num_features = X_train_padded.shape[2]\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. モデルの構築\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=(max_length, num_features)))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 5. モデルのコンパイルと学習\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_padded, y_test)\n",
    ")\n",
    "\n",
    "# 6. モデルの評価\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 7. 予測と結果の表示\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 8. 特徴量の重要度評価（Permutation Feature Importance）\n",
    "\n",
    "# 8.1 ベースラインの性能を計算\n",
    "# テストデータでの予測\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# ベースラインのAccuracyを計算\n",
    "baseline_accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "print(f'Baseline Accuracy: {baseline_accuracy:.4f}')\n",
    "\n",
    "# 8.2 特徴量ごとの重要度を計算\n",
    "num_features = X_test_padded.shape[2]\n",
    "feature_importances = []\n",
    "\n",
    "for feature_idx in range(num_features):\n",
    "    # テストデータをコピー\n",
    "    X_test_permuted = X_test_padded.copy()\n",
    "\n",
    "    # 特徴量をランダムにシャッフル（サンプル間でシャッフル）\n",
    "    permuted_feature = X_test_permuted[:, :, feature_idx].reshape(X_test_permuted.shape[0], -1)\n",
    "    np.random.shuffle(permuted_feature)\n",
    "    X_test_permuted[:, :, feature_idx] = permuted_feature.reshape(X_test_permuted.shape[0], X_test_permuted.shape[1])\n",
    "\n",
    "    # シャッフル後のデータで予測\n",
    "    y_pred_permuted = model.predict(X_test_permuted)\n",
    "    y_pred_classes_permuted = np.argmax(y_pred_permuted, axis=1)\n",
    "\n",
    "    # シャッフル後のAccuracyを計算\n",
    "    permuted_accuracy = accuracy_score(y_true_classes, y_pred_classes_permuted)\n",
    "\n",
    "    # 性能の差を計算（重要度）\n",
    "    importance = baseline_accuracy - permuted_accuracy\n",
    "    feature_importances.append(importance)\n",
    "    print(f'Feature {feature_idx + 1} Importance: {importance:.4f}')\n",
    "\n",
    "# 8.3 特徴量重要度の可視化\n",
    "feature_names = [f'Feature {i+1}' for i in range(num_features)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X[7].columns, feature_importances)\n",
    "plt.xlabel('Decrease in Accuracy')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# SHAPの初期化\n",
    "shap.initjs()\n",
    "\n",
    "# GradientExplainerの作成\n",
    "explainer = shap.DeepExplainer(model, X_train)\n",
    "\n",
    "\n",
    "# テストデータに対するSHAP値の計算\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# 5. SHAP値の集約\n",
    "# 全クラスのSHAP値の絶対値を合計\n",
    "shap_values_sum = np.sum(np.abs(shap_values), axis=0)  # 形状: (サンプル数, タイムステップ数, 特徴量数)\n",
    "# サンプル数とタイムステップ数方向に平均\n",
    "mean_shap_values = np.mean(shap_values_sum, axis=(0, 1))  # 形状: (特徴量数,)\n",
    "\n",
    "# 6. 特徴量名の指定（必要に応じて変更）\n",
    "feature_names = [f'Feature {i+1}' for i in range(num_features)]\n",
    "\n",
    "# 7. 特徴量重要度のプロット\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, mean_shap_values)\n",
    "plt.xlabel('Mean Absolute SHAP Value')\n",
    "plt.title('Feature Importance based on SHAP values')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHbr81KP4EsX"
   },
   "outputs": [],
   "source": [
    "#  pickleファイルを読み込む\n",
    "with open('/Users/hinase/Downloads/Th-s/d_nakazawa_acc_check_segments4.7new.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "segx = data['d_nakazawa_check_segx']\n",
    "segy = data['d_nakazawa_check_segy']\n",
    "segz = data['d_nakazawa_check_segz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_data = mr.process_apple_watch_csv('/Users/hinase/Downloads/tri/MotionData_20240819_205216.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple sine wave as an example of a waveform or signal\n",
    "x = np.linspace(0, 10, 1000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Define the sections to be colored\n",
    "color_sections = [(200, 300, 'red'), (450, 550, 'blue'), (700, 800, 'red')]\n",
    "\n",
    "# Plot the entire waveform in gray\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(x, y, color='gray', linewidth=3)\n",
    "\n",
    "# Highlight the specified sections with the chosen colors\n",
    "for start, end, color in color_sections:\n",
    "    plt.plot(x[start:end], y[start:end], color=color, linewidth=5)\n",
    "\n",
    "# Add an arrow to indicate the flow of data\n",
    "# plt.annotate('', xy=(10, 0), xytext=(0, 0),\n",
    "#              arrowprops=dict(facecolor='black', shrink=0.05, width=2))\n",
    "plt.xticks(color=\"None\")\n",
    "plt.yticks(color=\"None\")\n",
    "plt.tick_params(length=0)\n",
    "# Remove axes for a cleaner look\n",
    "plt.axis('off')\n",
    "plt.savefig(\"/Users/hinase/Downloads/plt.svg\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cairosvg\n",
    "\n",
    "# SVGファイルのパス\n",
    "input_svg_path = \"/Users/hinase/Downloads/ss.svg\"\n",
    "# 出力するEPSファイルのパス\n",
    "output_eps_path = \"/Users/hinase/Downloads/ss.eps\"\n",
    "\n",
    "# SVGをEPSに変換\n",
    "cairosvg.svg2eps(url=input_svg_path, write_to=output_eps_path)\n",
    "\n",
    "print(f\"SVG画像がEPS形式で '{output_eps_path}' に保存されました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import openpyxl\n",
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "\n",
    "folder_path = '/Users/hinase/Downloads/folder3'\n",
    "folder_2_path = '/Users/hinase/Downloads/folder4'\n",
    "\n",
    "\n",
    "# Excelファイル名リストを自然順で取得\n",
    "files = natsorted(os.listdir(folder_path))\n",
    "\n",
    "#ファイル名リストをfor文でまわして各ファイルの絶対パスを構築\n",
    "for filename in files:\n",
    "    filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "    # xlsxファイルにアクセス→先頭シートのオブジェクトを取得\n",
    "    wb = openpyxl.load_workbook(filepath)\n",
    "    ws_name = wb.sheetnames[0]\n",
    "    ws = wb[ws_name]\n",
    "\n",
    "    # csvに変換して、folder2に保存\n",
    "    savecsv_path = os.path.join(folder_2_path, filename.rstrip(\".xlsx\")+\".csv\")\n",
    "    with open(savecsv_path, 'w', newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in ws.rows:\n",
    "            writer.writerow([cell.value for cell in row])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
