{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGJb1ScIJE-J"
   },
   "outputs": [],
   "source": [
    "# Google Colabでライブラリをアップロードする\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI3b2MALJQG9"
   },
   "outputs": [],
   "source": [
    "# Google Colabでドライブのデータを使う\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5P9aOSopJUiL"
   },
   "outputs": [],
   "source": [
    "# Google Colabでライブラリをインストールする\n",
    "!pip install japanize_matplotlib bottleneck tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1Lq-ZkyCJJbu"
   },
   "outputs": [],
   "source": [
    "# 自作関数\n",
    "import MasterResearchFunction as mr\n",
    "\n",
    "# 基本ライブラリ\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import statistics\n",
    "from datetime import datetime, timedelta\n",
    "from decimal import Decimal\n",
    "\n",
    "# 数値計算とデータ処理\n",
    "import bottleneck as bn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 機械学習ライブラリ\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# ディープラーニングライブラリ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Add,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling1D,\n",
    "    Input,\n",
    "    LayerNormalization,\n",
    "    LSTM,\n",
    "    Masking,\n",
    "    MaxPooling1D,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# プロットと可視化\n",
    "import japanize_matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# その他のライブラリ\n",
    "from fastdtw import fastdtw\n",
    "from scipy import signal, stats\n",
    "from scipy.interpolate import interp1d, UnivariateSpline\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from tslearn.metrics import dtw_path\n",
    "import openpyxl\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定数の定義\n",
    "NAMES = ['sakamoto', 'watabe', 'kotera', 'nakazawa', 'mizuno', 'todaka', 'takagawa', 'uchida']\n",
    "LABELS = ['circle', 'cross', 'tri', 'check']\n",
    "COLUMNS_TO_DROP = [\n",
    "    'Sensor', 'Participant name', 'Event', 'Event value',\n",
    "    'Eye movement type', 'Eye movement type index', 'Ungrouped',\n",
    "    'Validity left', 'Validity right', 'Gaze event duration', 'Gaze2D_Distance',\n",
    "    'Fixation_Distance', 'Gaze3D_Distance', 'Pupil_Diameter_Change',\n",
    "    'GazeDirection_Distance', 'PupilPosition_Distance'\n",
    "]\n",
    "BASE_DIR = \"/Users/hinase/CodeChord/MasterResearch/datasets/new\"\n",
    "Hz = 100\n",
    "NON_GESTURE = 160\n",
    "#各被験者がジェスチャを行った順番(NAMES順)\n",
    "ROUTINE_LABELS = [\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 4, 3, 1],\n",
    "    [4, 3, 1, 2],\n",
    "    [2, 4, 3, 1],\n",
    "    [1, 4, 3, 2],\n",
    "    [3, 2, 1, 4],\n",
    "    [3, 1, 2, 4],\n",
    "    [2, 3, 1, 4]\n",
    "]\n",
    "# 数字からラベルへの変換マッピング\n",
    "label_map = {1: 'circle', 2: 'cross', 3: 'tri', 4: 'check'}\n",
    "# ラベル辞書を作成\n",
    "routine_label_dict = {name: [label_map[num] for num in labels] for name, labels in zip(NAMES, ROUTINE_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ジェスチャの教師データの読み込み\n",
    "trimmed_data_dict = mr.load_trimmed_gesture_data(NAMES, BASE_DIR)\n",
    "first_gesture_data_dict = mr.load_trimmed_first_gesture_data(NAMES, BASE_DIR)\n",
    "#教師データの最小の長さと最大の長さを格納\n",
    "length_extremes_dict = mr.calculate_length_extremes(trimmed_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#実験ルーティンのデータの読み込み\n",
    "routine_data_dict = {}\n",
    "eye_routine_data_dict = {}\n",
    "for name in NAMES:\n",
    "    print(f\"Processing {name}...\")\n",
    "    # データの読み込みと前処理\n",
    "    routine_data_dict[name] = mr.process_apple_watch_csv(f'{BASE_DIR}/{name}/motion/{name}_new_routine.csv')\n",
    "    eye_routine_data_dict[name] = mr.process_tobii_csv(f'{BASE_DIR}/{name}/eye/{name}_eye_new_routine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ルーティン中の各ジェスチャの実行部分のインデックスの抽出\n",
    "grouped_intervals = {}\n",
    "group_label_mapping = {}\n",
    "\n",
    "for name in NAMES:\n",
    "    print(f\"Processing {name}...\")\n",
    "    # データの読み込みと前処理\n",
    "    a = mr.find_true_intervals(routine_data_dict[name])\n",
    "\n",
    "    # インターバルの開始インデックスを取得\n",
    "    start_indices = np.array([interval[0] for interval in a]).reshape(-1, 1)\n",
    "\n",
    "    # 階層的クラスタリング\n",
    "    Z = linkage(start_indices, method='ward')\n",
    "    cluster_labels = fcluster(Z, t=4, criterion='maxclust')  # クラスタ数を4に固定\n",
    "\n",
    "    # クラスタごとにインターバルをグループ化\n",
    "    interval_groups = {i: [] for i in range(1, 5)}\n",
    "    for interval, label in zip(a, cluster_labels):\n",
    "        interval_groups[label].append(interval)\n",
    "\n",
    "    # グループの順序をインターバルの開始インデックスに基づいてソート\n",
    "    sorted_groups = sorted(interval_groups.items(), key=lambda x: min(interval[0] for interval in x[1]))\n",
    "    sorted_interval_groups = {i + 1: group for i, (_, group) in enumerate(sorted_groups)}\n",
    "\n",
    "    # グループを保存\n",
    "    grouped_intervals[name] = sorted_interval_groups\n",
    "\n",
    "    # グループとラベルを関連付け\n",
    "    group_label_mapping[name] = {label: sorted_interval_groups[group_id]\n",
    "                                for group_id, label in enumerate(routine_label_dict[name], start=1)}\n",
    "\n",
    "    # 結果を出力\n",
    "    print(f\"\\n{name} のグループとラベル対応:\")\n",
    "    for label, intervals in group_label_mapping[name].items():\n",
    "        print(f\"  {label}: {intervals}\")\n",
    "\n",
    "# 'nakazawa' の 'check' に対応する最初のインデックスの組を削除\n",
    "if 'nakazawa' in group_label_mapping and 'check' in group_label_mapping['nakazawa']:\n",
    "    if len(group_label_mapping['nakazawa']['check']) > 0:\n",
    "        removed_interval = group_label_mapping['nakazawa']['check'].pop(0)\n",
    "        print(f\"'nakazawa' の 'check' から削除されたインターバル: {removed_interval}\")\n",
    "    else:\n",
    "        print(\"'nakazawa' の 'check' に削除するインターバルがありません。\")\n",
    "else:\n",
    "    print(\"'nakazawa' または 'check' に対応するデータが見つかりません。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ジェスチャ実行時の-5秒から+5秒の範囲を抜き出す\n",
    "extracted_data_dict = {}\n",
    "\n",
    "for name in NAMES:\n",
    "    print(f\"Processing {name}...\")\n",
    "    extracted_data_dict[name] = {}\n",
    "\n",
    "    for label in LABELS:\n",
    "        print(f\"Processing label: {label}...\")\n",
    "\n",
    "        # 該当するインターバルを取得\n",
    "        intervals = group_label_mapping[name][label]\n",
    "\n",
    "        # 最初のインデックスと最後のインデックスを計算\n",
    "        start_idx = intervals[0][0] - 500\n",
    "        end_idx = intervals[-1][1] + 500\n",
    "\n",
    "        # インデックスの範囲を調整（負のインデックス防止）\n",
    "        start_idx = max(0, start_idx)\n",
    "        end_idx = min(len(routine_data_dict[name]), end_idx)\n",
    "\n",
    "        # データを抜き出し\n",
    "        extracted_data = routine_data_dict[name].iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "\n",
    "        # 抜き出したデータを辞書に格納\n",
    "        extracted_data_dict[name][label] = extracted_data\n",
    "\n",
    "        print(f\"Extracted data for {name} - {label}: start={start_idx}, end={end_idx}, length={len(extracted_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期化\n",
    "first_gesture = {}\n",
    "# 各被験者とジェスチャに対して最初のデータを抽出\n",
    "for name in NAMES:\n",
    "    first_gesture[name] = {}  # 各被験者の辞書を初期化\n",
    "\n",
    "    for label in LABELS:\n",
    "        print(f\"Processing {name} - {label}...\")\n",
    "        try:\n",
    "            # インデックス範囲を取得\n",
    "            start_idx = group_label_mapping[name][label][0][0]\n",
    "            end_idx = group_label_mapping[name][label][0][1]\n",
    "\n",
    "            # データを抽出\n",
    "            extracted_data = routine_data_dict[name].iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "\n",
    "            # 辞書に格納\n",
    "            first_gesture[name][label] = extracted_data\n",
    "            print(f\"Extracted data for {name} - {label} (Index range: {start_idx}-{end_idx})\")\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}. Skipping {name} - {label}.\")\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError: {e}. Skipping {name} - {label}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = mr.filter_and_combine_segments(b, Hz, min_time=length_extremes_dict[NAMES[namae]][LABELS[jesutya]]['min_length']*0.5/Hz, max_time=length_extremes_dict[NAMES[namae]][LABELS[jesutya]]['max_length']*2.5/Hz, overlap_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/hinase/Downloads/spring_results/watabe_spring_results.pkl', 'rb') as f:\n",
    " = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. データの読み込みと前処理\n",
    "X_filled, y_labels, true_labels, data_dict, label_dict, true_label_dict = mr.preprocess_gesture_data(\n",
    "    NAMES,\n",
    "    LABELS,\n",
    "    COLUMNS_TO_DROP,\n",
    "    NON_GESTURE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.check_timestamp_gaps(NAMES, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. データのフィルタリングとスケーリング\n",
    "# 2. 前処理とフィルタリングの適用\n",
    "filtered_X, filtered_y, filtered_true = mr.preprocess_and_filter_sequences(\n",
    "    X_filled,\n",
    "    y_labels,\n",
    "    true_labels,\n",
    "    COLUMNS_TO_DROP,\n",
    "    sampling_rate=50.0,\n",
    "    max_gap_seconds=0.1,\n",
    "    max_nan_seconds=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_X[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的なデータの確認\n",
    "print(f\"処理後のデータ数: {len(filtered_X)}\")\n",
    "print(f\"ラベル配列の長さ（y_labels_scaled）: {len(filtered_y)}\")\n",
    "print(f\"真偽ラベル配列の長さ（true_labels_scaled）: {len(filtered_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = []\n",
    "scaler = StandardScaler()\n",
    "for i in range(len(filtered_X)):\n",
    "    X_scaled.append(scaler.fit_transform(filtered_X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ラベルのエンコーディング（2値分類用に修正）\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(filtered_true)  # 0または1にエンコード\n",
    "\n",
    "# 5. データの分割（パディングの前に行う）\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seq in enumerate(X_scaled):\n",
    "    print(f\"Sequence {i}: shape {seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 訓練データとテストデータでそれぞれ最大シーケンス長を計算\n",
    "max_length_train = max(len(seq) for seq in X_train_raw)\n",
    "max_length_test = max(len(seq) for seq in X_test_raw)\n",
    "max_length = max(max_length_train, max_length_test)\n",
    "\n",
    "# 7. シーケンスのパディング\n",
    "X_train_padded = pad_sequences(X_train_raw, maxlen=max_length, padding='post', value=0.0, dtype='float32')\n",
    "X_test_padded = pad_sequences(X_test_raw, maxlen=max_length, padding='post', value=0.0, dtype='float32')\n",
    "\n",
    "# 8. パディング後のデータに NaN が含まれていないか確認\n",
    "print('NaN in X_train_padded:', np.isnan(X_train_padded).any())\n",
    "print('NaN in X_test_padded:', np.isnan(X_test_padded).any())\n",
    "\n",
    "# 必要に応じて NaN を 0 で置換\n",
    "if np.isnan(X_train_padded).any():\n",
    "    X_train_padded = np.nan_to_num(X_train_padded, nan=0.0)\n",
    "if np.isnan(X_test_padded).any():\n",
    "    X_test_padded = np.nan_to_num(X_test_padded, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    固定のサイン・コサイン位置エンコーディングを生成する関数。\n",
    "\n",
    "    Parameters:\n",
    "    - max_len (int): シーケンスの最大長\n",
    "    - d_model (int): 埋め込み次元数\n",
    "\n",
    "    Returns:\n",
    "    - pos_encoding (np.array): 位置エンコーディング行列\n",
    "    \"\"\"\n",
    "    angle_rads = np.arange(max_len)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :]//2)) / np.float32(d_model))\n",
    "\n",
    "    # 偶数次元はsin、奇数次元はcos\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]  # (1, max_len, d_model)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    \"\"\"\n",
    "    Transformerエンコーダーブロックを構築する関数。\n",
    "    \"\"\"\n",
    "    # 自己注意機構（マスクは自動的に適用される）\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs + x)\n",
    "\n",
    "    # フィードフォワードネットワーク\n",
    "    x_ff = Dense(ff_dim, activation='relu')(x)\n",
    "    x_ff = Dense(inputs.shape[-1])(x_ff)\n",
    "    x_ff = Dropout(dropout)(x_ff)\n",
    "    out = LayerNormalization(epsilon=1e-6)(x + x_ff)\n",
    "    return out\n",
    "\n",
    "def build_transformer_model(max_length, num_features, head_size, num_heads, ff_dim, num_transformer_blocks, dropout):\n",
    "    \"\"\"\n",
    "    Transformerベースの2値分類モデルを構築する関数。\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(max_length, num_features))\n",
    "\n",
    "    # マスキングレイヤーの追加（パディング値が0の場合）\n",
    "    x = Masking(mask_value=0.0)(inputs)\n",
    "\n",
    "    # 位置エンコーディングの追加\n",
    "    pos_encoding = get_positional_encoding(max_length, num_features)\n",
    "    x = x + pos_encoding[:,:max_length, :]\n",
    "\n",
    "    # Transformerエンコーダーブロックの追加\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    # グローバルプーリング\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    # 出力層\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの構築\n",
    "model = build_transformer_model(\n",
    "    max_length=max_length,\n",
    "    num_features=X_train_padded.shape[2],\n",
    "    head_size=64,\n",
    "    num_heads=8,\n",
    "    ff_dim=128,\n",
    "    num_transformer_blocks=1,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# モデルのサマリー表示\n",
    "model.summary()\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# モデルの訓練\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的なデータの確認\n",
    "print(f\"処理後のデータ数: {len(filtered_X)}\")\n",
    "print(f\"ラベル配列の長さ（y_labels_scaled）: {len(filtered_y)}\")\n",
    "print(f\"真偽ラベル配列の長さ（true_labels_scaled）: {len(filtered_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線をプロット\n",
    "def plot_learning_curve(history):\n",
    "    # 損失をプロット\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # 精度をプロット\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 関数を呼び出してプロット\n",
    "plot_learning_curve(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# 予測確率を取得\n",
    "y_pred_proba = model.predict(X_test_padded)\n",
    "\n",
    "# ROC曲線を計算\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# AUCスコアを計算\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# ROC曲線をプロット\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess', color='red')  # ランダムな分類の基準線\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. モデルの評価\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 5. 予測と結果の表示\n",
    "y_pred_prob = model.predict(X_test_padded)\n",
    "y_pred_classes = (y_pred_prob > 0.5).astype(int).flatten()  # 閾値0.5でクラスを決定\n",
    "y_true_classes = y_test\n",
    "\n",
    "# ラベルを逆変換（元のクラス名に戻す場合）\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "\n",
    "# 分類レポートの表示\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定数の定義\n",
    "NAMES = ['sakamoto', 'watabe', 'kotera', 'nakazawa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#実験ルーティンのデータの読み込み\n",
    "spring_index_dict = {}\n",
    "for name in NAMES:\n",
    "  print(f\"Processing {name}...\")\n",
    "  spring_index_dict[name] = {}\n",
    "  with open(f'/Users/hinase/CodeChord/MasterResearch/datasets/new/spring_results/{name}_spring_results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "  for label in LABELS:\n",
    "    spring_index_dict[name][label] = []\n",
    "    spring_index_dict[name][label] = results[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/Users/hinase/CodeChord/MasterResearch/datasets/new\"\n",
    "SPRING_RESULTS_DIR = os.path.join(BASE_DIR, \"spring_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各種結果を格納する辞書\n",
    "spring_index_dict = {}\n",
    "spring_final_index_dict = {}\n",
    "extracted_eye_data_dict = {}\n",
    "\n",
    "# 一連の処理をループ内で実行\n",
    "for name in NAMES:\n",
    "    print(f\"Processing {name}...\")\n",
    "    spring_index_dict[name] = {}\n",
    "    spring_final_index_dict[name] = {}\n",
    "    extracted_eye_data_dict[name] = {}\n",
    "\n",
    "    # SPRING結果を読み込み\n",
    "    spring_results_path = os.path.join(SPRING_RESULTS_DIR, f\"{name}_spring_results.pkl\")\n",
    "    with open(spring_results_path, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    for label in LABELS:\n",
    "        # SPRING結果をフィルタリング\n",
    "        spring_index_dict[name][label] = results[label]\n",
    "        spring_final_index_dict[name][label] = mr.filter_and_combine_segments(\n",
    "            spring_index_dict[name][label],\n",
    "            Hz,\n",
    "            min_time=length_extremes_dict[name][label]['min_length'] * 0.5 / Hz,\n",
    "            max_time=length_extremes_dict[name][label]['max_length'] * 2.5 / Hz,\n",
    "            overlap_count=2\n",
    "        )\n",
    "\n",
    "        # タイムスタンプを算出\n",
    "        filtered_results = mr.extract_timestamp_from_overlap(\n",
    "            routine_data_dict[name], spring_final_index_dict[name][label]\n",
    "        )\n",
    "\n",
    "        # 視線データを抽出\n",
    "        extracted_eye_data = []\n",
    "        for start_time, end_time in filtered_results:\n",
    "            extracted_eye_data.append(\n",
    "                eye_routine_data_dict[name][\n",
    "                    (eye_routine_data_dict[name]['Timestamp'] >= start_time) &\n",
    "                    (eye_routine_data_dict[name]['Timestamp'] <= end_time)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # 抽出した視線データを辞書に格納\n",
    "        extracted_eye_data_dict[name][label] = extracted_eye_data\n",
    "\n",
    "# 結果を確認\n",
    "for name, labels_data in extracted_eye_data_dict.items():\n",
    "    for label, eye_data in labels_data.items():\n",
    "        print(f\"{name} - {label}: {len(eye_data)} segments extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# プロット関数\n",
    "def plot_with_highlights(routine_data_dict, group_label_mapping, spring_final_index_dict, name, label):\n",
    "    \"\"\"\n",
    "    特定の被験者とジェスチャーのルーティンデータをプロットし、\n",
    "    group_label_mappingとspring_final_index_dictのインデックス範囲をハイライト。\n",
    "\n",
    "    Parameters:\n",
    "    - routine_data_dict (dict): 被験者ごとのルーティンデータの辞書。\n",
    "    - group_label_mapping (dict): 被験者ごとのグループとラベルの対応辞書。\n",
    "    - spring_final_index_dict (dict): SPRINGによるインデックスの辞書。\n",
    "    - name (str): 被験者名。\n",
    "    - label (str): ジェスチャーラベル。\n",
    "    \"\"\"\n",
    "    data = routine_data_dict[name]\n",
    "\n",
    "    # プロット\n",
    "    plt.figure(figsize=(100, 8))\n",
    "    plt.plot(data['EuclideanNorm'], label='EuclideanNorm', color='blue', alpha=0.9)\n",
    "\n",
    "    # ラベルごとのインデックス範囲を黄色でハイライト (group_label_mapping)\n",
    "    if label in group_label_mapping[name]:\n",
    "        intervals = group_label_mapping[name][label]\n",
    "        for start, end in intervals:\n",
    "            plt.axvspan(start, end, color='yellow', alpha=0.3, label=f'{label} (manual)' if start == intervals[0][0] else \"\")\n",
    "\n",
    "    # SPRINGのインデックス範囲を水色でハイライト (spring_final_index_dict)\n",
    "    if label in spring_final_index_dict[name]:\n",
    "        intervals = spring_final_index_dict[name][label]\n",
    "        for start, end in intervals:\n",
    "            plt.axvspan(start, end, color='cyan', alpha=0.4, label=f'{label} (SPRING)' if start == intervals[0][0] else \"\")\n",
    "\n",
    "    # プロットの設定\n",
    "    plt.title(f\"{name}'s Routine Data ({label}) with Highlighted Gesture Segments\", fontsize=16)\n",
    "    plt.xlabel(\"Time Step\", fontsize=14)\n",
    "    plt.ylabel(\"Acceleration Value\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 被験者ごと、ラベルごとのプロットを実行\n",
    "for name in NAMES:\n",
    "    for label in LABELS:\n",
    "        print(f\"Plotting {name}'s data for {label}...\")\n",
    "        plot_with_highlights(routine_data_dict, group_label_mapping, spring_final_index_dict, name, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_prepare_input(extracted_eye_data_dict, scaler, max_length, COLUMNS_TO_DROP, sampling_rate=50.0, max_gap_seconds=5.0):\n",
    "    \"\"\"\n",
    "    extracted_eye_data_dict に含まれるデータを前処理し、標準化およびパディングを適用する関数。\n",
    "\n",
    "    Parameters:\n",
    "    - extracted_eye_data_dict (dict): 視線データの辞書\n",
    "    - scaler (StandardScaler): フィット済みの標準化スケーラー\n",
    "    - max_length (int): パディングの最大長\n",
    "    - COLUMNS_TO_DROP (list): 削除するカラムのリスト\n",
    "    - sampling_rate (float): サンプリング周波数（Hz）\n",
    "    - max_gap_seconds (float): 最大欠損許容時間（秒）\n",
    "\n",
    "    Returns:\n",
    "    - dict: 標準化およびパディング後のデータ辞書\n",
    "    \"\"\"\n",
    "    X_padded = {}\n",
    "\n",
    "    for name, labels_data in extracted_eye_data_dict.items():\n",
    "        X_padded[name] = {}\n",
    "        for label, data_list in labels_data.items():\n",
    "            X_padded[name][label] = []\n",
    "            for i, df in enumerate(data_list):\n",
    "                try:\n",
    "                    print(f\"Processing {name} - {label} - Segment {i}...\")\n",
    "                    # データの前処理を適用\n",
    "                    processed_df = mr.preprocess_sequence(df, COLUMNS_TO_DROP, sampling_rate=sampling_rate, max_gap_seconds=max_gap_seconds)\n",
    "                    # タイムスタンプを削除\n",
    "                    processed_df = processed_df.drop(columns=['Timestamp'], errors='ignore')\n",
    "                    # 標準化\n",
    "                    scaled_data = scaler.transform(processed_df.values)\n",
    "                    # 結果をリストに追加\n",
    "                    X_padded[name][label].append(scaled_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {name} - {label} - Segment {i}: {e}\")\n",
    "\n",
    "            # パディング\n",
    "            print(f\"Padding {name} - {label}...\")\n",
    "            padded_data = pad_sequences(\n",
    "                X_padded[name][label], maxlen=max_length, padding='post', value=0.0, dtype='float32'\n",
    "            )\n",
    "            # NaN の確認と置換\n",
    "            if np.isnan(padded_data).any():\n",
    "                print(f\"NaN detected in {name} - {label}. Replacing with 0.\")\n",
    "                padded_data = np.nan_to_num(padded_data, nan=0.0)\n",
    "\n",
    "            # パディング後のデータを保存\n",
    "            X_padded[name][label] = padded_data\n",
    "\n",
    "    return X_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_eye_data_dict に preprocess_and_prepare_input を適用\n",
    "X_padded = preprocess_and_prepare_input(\n",
    "    extracted_eye_data_dict, scaler, max_length, COLUMNS_TO_DROP, sampling_rate=50.0, max_gap_seconds=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果を格納する辞書\n",
    "predictions_dict = {}\n",
    "\n",
    "# 各データに対して予測を実行\n",
    "for name, labels_data in X_padded.items():\n",
    "    predictions_dict[name] = {}\n",
    "    for label, data in labels_data.items():\n",
    "        print(f\"Predicting for {name} - {label}...\")\n",
    "        try:\n",
    "            # model.predict を使用して予測を行う\n",
    "            predictions = model.predict(data)\n",
    "            # 予測結果を辞書に格納\n",
    "            predictions_dict[name][label] = predictions\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting {name} - {label}: {e}\")\n",
    "\n",
    "# 予測結果の出力\n",
    "for name, labels_data in predictions_dict.items():\n",
    "    for label, predictions in labels_data.items():\n",
    "        print(f\"{name} - {label}:\")\n",
    "        print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果を格納する辞書\n",
    "predicted_labels_dict = {}\n",
    "\n",
    "for name, labels_data in X_padded.items():\n",
    "    predicted_labels_dict[name] = {}\n",
    "    for label, data in labels_data.items():\n",
    "        print(f\"Predicting and classifying for {name} - {label}...\")\n",
    "        try:\n",
    "            # モデルで予測\n",
    "            y_pred_prob = model.predict(data)\n",
    "            # 閾値0.5でクラスを決定\n",
    "            y_pred_classes = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "            # クラスをラベルに逆変換\n",
    "            y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "            # 予測ラベルを辞書に格納\n",
    "            predicted_labels_dict[name][label] = y_pred_labels\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting for {name} - {label}: {e}\")\n",
    "\n",
    "# 分類結果の出力\n",
    "for name, labels_data in predicted_labels_dict.items():\n",
    "    for label, predictions in labels_data.items():\n",
    "        print(f\"{name} - {label}: Predicted Labels:\")\n",
    "        print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'gesture' に対応するインデックスだけを抜き出す辞書\n",
    "filtered_spring_index_dict = {}\n",
    "\n",
    "for name, labels_data in spring_final_index_dict.items():\n",
    "    filtered_spring_index_dict[name] = {}\n",
    "    for label, intervals in labels_data.items():\n",
    "        print(f\"Filtering {name} - {label}...\")\n",
    "\n",
    "        # 'gesture' に対応するインデックスだけを抽出\n",
    "        filtered_intervals = [\n",
    "            interval for interval, prediction in zip(intervals, predicted_labels_dict[name][label])\n",
    "            if prediction == 'gesture'\n",
    "        ]\n",
    "\n",
    "        # 抽出結果を新しい辞書に格納\n",
    "        filtered_spring_index_dict[name][label] = filtered_intervals\n",
    "\n",
    "# 結果の確認\n",
    "for name, labels_data in filtered_spring_index_dict.items():\n",
    "    for label, intervals in labels_data.items():\n",
    "        print(f\"{name} - {label}: {len(intervals)} intervals labeled as 'gesture'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 被験者ごと、ラベルごとのプロットを実行\n",
    "for name in NAMES:\n",
    "    for label in LABELS:\n",
    "        print(f\"Plotting {name}'s data for {label}...\")\n",
    "        plot_with_highlights(routine_data_dict, group_label_mapping, filtered_spring_index_dict, name, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列の作成と可視化\n",
    "conf_mat = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルから予測値を取得する関数をSHAPに渡す\n",
    "explainer = shap.Explainer(model, X_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (max_length, num_features)\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Masking(mask_value=0.)(inputs)\n",
    "\n",
    "# num_featuresを定義（入力データの特徴量の次元数）\n",
    "num_features = input_shape[1]\n",
    "\n",
    "# Transformerブロックの定義\n",
    "def transformer_block(x, num_heads, key_dim, ff_dim, rate=0.1):\n",
    "    # マルチヘッド注意機構\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "\n",
    "    # 出力次元をnum_featuresに変換\n",
    "    attn_output = Dense(num_features)(attn_output)\n",
    "\n",
    "    out1 = Add()([x, attn_output])\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "\n",
    "    # フィードフォワードネットワーク\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "\n",
    "    # 出力次元をnum_featuresに変換\n",
    "    ffn_output = Dense(num_features)(ffn_output)\n",
    "\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    out2 = Add()([out1, ffn_output])\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out2)\n",
    "    return out2\n",
    "\n",
    "# Transformerブロックの適用\n",
    "x = transformer_block(x, num_heads=4, key_dim=64, ff_dim=128)\n",
    "\n",
    "# プーリングと出力層\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "# モデルの作成\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. モデルのコンパイルと学習\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_padded, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. モデルの評価\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "# 7. 予測と結果の表示\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 8. 特徴量の重要度評価（Permutation Feature Importance）\n",
    "\n",
    "# 8.1 ベースラインの性能を計算\n",
    "# テストデータでの予測\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# ベースラインのAccuracyを計算\n",
    "baseline_accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "print(f'Baseline Accuracy: {baseline_accuracy:.4f}')\n",
    "\n",
    "# 8.2 特徴量ごとの重要度を計算\n",
    "num_features = X_test_padded.shape[2]\n",
    "feature_importances = []\n",
    "\n",
    "for feature_idx in range(num_features):\n",
    "    # テストデータをコピー\n",
    "    X_test_permuted = X_test_padded.copy()\n",
    "\n",
    "    # 特徴量をランダムにシャッフル（サンプル間でシャッフル）\n",
    "    permuted_feature = X_test_permuted[:, :, feature_idx].reshape(X_test_permuted.shape[0], -1)\n",
    "    np.random.shuffle(permuted_feature)\n",
    "    X_test_permuted[:, :, feature_idx] = permuted_feature.reshape(X_test_permuted.shape[0], X_test_permuted.shape[1])\n",
    "\n",
    "    # シャッフル後のデータで予測\n",
    "    y_pred_permuted = model.predict(X_test_permuted)\n",
    "    y_pred_classes_permuted = np.argmax(y_pred_permuted, axis=1)\n",
    "\n",
    "    # シャッフル後のAccuracyを計算\n",
    "    permuted_accuracy = accuracy_score(y_true_classes, y_pred_classes_permuted)\n",
    "\n",
    "    # 性能の差を計算（重要度）\n",
    "    importance = baseline_accuracy - permuted_accuracy\n",
    "    feature_importances.append(importance)\n",
    "    print(f'Feature {feature_idx + 1} Importance: {importance:.4f}')\n",
    "\n",
    "# 8.3 特徴量重要度の可視化\n",
    "feature_names = [f'Feature {i+1}' for i in range(num_features)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X[7].columns, feature_importances)\n",
    "plt.xlabel('Decrease in Accuracy')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.9.19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
